{
  "hash": "3022cd07fbbbd3d42fa73b56d9d7e331",
  "result": {
    "engine": "jupyter",
    "markdown": "# Automatic Differentiation\n\nAutomatic differentiation (AD) is a technique for computing exact derivatives of functions specified by computer programs. Unlike symbolic differentiation (which manipulates mathematical expressions) or numerical differentiation (which uses finite differences), AD exploits the fact that every program, no matter how complex, executes a sequence of elementary operations. By applying the chain rule systematically to these operations, AD computes derivatives to machine precision.\n\n## Why AD Matters for Structural Estimation\n\nIn structural econometrics, we frequently need gradients for:\n\n- Optimization (MLE, GMM, minimum distance)\n    - Gradient-free methods such as Nelder-Mead are popular, of course, but are less efficient\n- Computing standard errors\n- Solving models with equilibrium conditions (using Newton's method, for example)\n\nHand-coding derivatives is tedious and error-prone. Finite differences are slow (requiring $O(n)$ function evaluations for an $n$-dimensional gradient) and can be numerically unstable. AD provides exact gradients efficiently.\n\n## Forward Mode vs Reverse Mode\n\nAD comes in two flavors:\n\n**Forward mode** propagates derivatives forward through the computation. For a function $f: \\mathbb{R}^n \\to \\mathbb{R}^m$, computing the full Jacobian requires $n$ forward passes. This is efficient when $n \\ll m$.\n\n**Reverse mode** propagates derivatives backward (like backpropagation in neural networks). Computing the full Jacobian requires $m$ reverse passes. This is efficient when $m \\ll n$.\n\nFor most estimation problems, we have a scalar objective ($m = 1$) and many parameters ($n$ large), so reverse mode is typically preferred. *In my experience however*, I have had more success writing code that is compatible with forward differencing. You will learn from experience that these tools can be fussy.\n\n## AD in Julia\n\nJulia's AD ecosystem is excellent. The main packages are:\n\n### ForwardDiff.jl\n\nForward-mode AD. Simple and robust, works out-of-the-box for most pure Julia code.\n\n::: {#4113d652 .cell execution_count=1}\n``` {.julia .cell-code}\nusing ForwardDiff\n\nf(x) = sum(x.^2)\nx = [1.0, 2.0, 3.0]\n\n# Gradient\nForwardDiff.gradient(f, x)\n```\n\n::: {.cell-output .cell-output-display execution_count=2}\n```\n3-element Vector{Float64}:\n 2.0\n 4.0\n 6.0\n```\n:::\n:::\n\n\n::: {#d4d83863 .cell execution_count=2}\n``` {.julia .cell-code}\n# Hessian\nForwardDiff.hessian(f, x)\n```\n\n::: {.cell-output .cell-output-display execution_count=3}\n```\n3×3 Matrix{Float64}:\n 2.0  0.0  0.0\n 0.0  2.0  0.0\n 0.0  0.0  2.0\n```\n:::\n:::\n\n\n### Enzyme.jl\n\nA high-performance AD engine that works at the LLVM level. Supports both forward and reverse mode. Often the fastest option, especially for code with loops and mutations.\n\n::: {#3ef76b29 .cell execution_count=3}\n``` {.julia .cell-code}\nusing Enzyme\n\nf(x) = x[1]^2 + sin(x[2])\n\nx = [1.0, 2.0]\ndx = zeros(2)\n\n# Reverse mode gradient\nEnzyme.autodiff(Reverse, f, Active, Duplicated(x, dx))\ndx\n```\n\n::: {.cell-output .cell-output-display execution_count=4}\n```\n2-element Vector{Float64}:\n  2.0\n -0.4161468365471424\n```\n:::\n:::\n\n\n### Zygote.jl\n\nA source-to-source reverse-mode AD system. Popular in machine learning (used by Flux.jl). Works well for array-heavy code but may struggle with control flow.\n\n::: {#9b5de75c .cell execution_count=4}\n``` {.julia .cell-code}\nusing Zygote\n\nf(x) = sum(x.^2)\nx = [1.0, 2.0, 3.0]\n\nZygote.gradient(f, x)\n```\n\n::: {.cell-output .cell-output-display execution_count=5}\n```\n([2.0, 4.0, 6.0],)\n```\n:::\n:::\n\n\n## Practical Recommendations\n\n1. **Start with ForwardDiff** for problems with few parameters (< 100). It's the most reliable.\n\n2. **Use Enzyme for performance-critical code**, especially if you have loops or in-place mutations.\n\n3. **Be aware of limitations**: AD systems can fail on code that uses certain constructs (try-catch, foreign function calls, some global variables). When in doubt, test that your gradients match finite differences:\n\n::: {#b6926482 .cell execution_count=5}\n``` {.julia .cell-code}\nusing ForwardDiff, FiniteDiff\n\nf(x) = log(1 + exp(x[1] * x[2])) + x[3]^2\nx = [1.0, 2.0, 3.0]\n\nad_grad = ForwardDiff.gradient(f, x)\nfd_grad = FiniteDiff.finite_difference_gradient(f, x)\n\nmaximum(abs.(ad_grad .- fd_grad))\n```\n\n::: {.cell-output .cell-output-display execution_count=6}\n```\n5.3512749786932545e-12\n```\n:::\n:::\n\n\n## Integration with Optimization\n\nMost Julia optimization packages accept AD gradients. Here's an example with `Optim.jl`:\n\n::: {#cff9a951 .cell execution_count=6}\n``` {.julia .cell-code}\nusing Optim, ForwardDiff\n\nrosenbrock(x) = (1 - x[1])^2 + 100*(x[2] - x[1]^2)^2\nx0 = [0.0, 0.0]\n\n# With automatic gradients via ForwardDiff\nresult = optimize(rosenbrock, x0, LBFGS(); autodiff = :forward)\nresult.minimizer\n```\n\n::: {.cell-output .cell-output-display execution_count=7}\n```\n2-element Vector{Float64}:\n 0.999999999999928\n 0.9999999999998559\n```\n:::\n:::\n\n\n## Example: Maximum Likelihood with `Optim.jl`\n\nConsider a simple probit model:\n\n$$ D = \\mathbf{1}\\{X\\beta - \\nu \\geq 0\\},\\qquad \\nu \\sim \\mathcal{N}(0,1) $$\n\nHere is code to simulate data for this model:\n\n::: {#fdcfd65a .cell execution_count=7}\n``` {.julia .cell-code}\nusing Random, Distributions\n\nfunction sim_data(X ; γ)\n    N = size(X,1)\n    ν = rand(Normal(),N)\n    D = (X * γ .- ν) .> 0\n    return D\nend\n\n# a quick test of the function:\nN = 1000\nX = [ones(N) 2*rand(Normal(),N)]\nγ = [0.1, 0.5]\nD = sim_data(X ; γ);\n```\n:::\n\n\nConsider the problem of estimating $\\gamma$ using maximum likelihood. We will establish the properties of this estimator in class. Here let's just focus on numerically how to attack the minimization problem. The log-likelihood of the data `D` given `X` is:\n\n$$ \\mathcal{L}(\\gamma) = \\sum_{n}l(D_{n}; X_{n},\\gamma) = \\sum_{n=1}^{N}D_{n}\\log(\\Phi(X\\gamma)) + (1-D_{n})\\log(1-\\Phi(X\\gamma)) $$\n\nLet's write up this likelihood function.\n\n::: {#0a7fb783 .cell execution_count=8}\n``` {.julia .cell-code}\nfunction log_likelihood(D,X,γ)\n    ll = 0.\n    for n in eachindex(D)\n        xg = X[n,1] * γ[1] + X[n,2] * γ[2] \n        if D[n]\n            ll += log(cdf(Normal(),xg))\n        else\n            ll += log(1-cdf(Normal(),xg))\n        end\n    end\n    return ll\nend\nlog_likelihood(D,X,[0.,0.])\n```\n\n::: {.cell-output .cell-output-display execution_count=9}\n```\n-693.1471805599322\n```\n:::\n:::\n\n\n### Numerical Optimization\n\nOptimization is most efficient when we have access to the first and second order derivatives of the function. There is a general class of hill-climbing (or descent in the case of minimization) algorithms that find new guesses $\\gamma_{k+1}$ given $\\gamma_{k}$ as:\n\n$$ \\gamma_{k+1} = \\gamma_{k} + \\lambda_{k}A_{k}\\frac{\\partial Q}{\\partial \\gamma} $$\n\nwhere $Q$ is the function being maximized (or minimized). $A_{k}$ defines a direction in which to search (providing weights on the derivatives) and $\\lambda_{k}$ is a scalar variable known as a step-size which is often calculated optimally in each iteration $k$. For Newton's method, the matrix $A_{k}$ is the inverse of the Hessian of the objective function $Q$. Since the hessian can sometimes be expensive to calculate, other methods use approximations to the Hessian that are cheaper to compute.\n\nSince we have a simple model, we can calculate derivatives relatively easily. Below we'll compare a hard-coded derivative to this automatic differentiation.\n\n::: {#446f711e .cell execution_count=9}\n``` {.julia .cell-code}\nusing ForwardDiff\n\nfunction deriv_ll(D,X,γ)\n    dll = zeros(2)\n    for n in eachindex(D)\n        xg = X[n,1] * γ[1] + X[n,2] * γ[2] \n        if D[n]\n            dl = pdf(Normal(),xg) / cdf(Normal(),xg)\n        else\n            dl = - pdf(Normal(),xg) / (1 - cdf(Normal(),xg))\n        end\n        dll[1] += X[n,1] * dl\n        dll[2] += X[n,2] * dl            \n    end\n    return dll\nend\ndx = zeros(2)\n# forward mode\nauto_deriv_ll(D,X,γ) = ForwardDiff.gradient(x->log_likelihood(D,X,x),γ)\n# reverse mode\nauto_deriv2_ll(D,X,γ,dx) = Enzyme.autodiff(Reverse, x->log_likelihood(D,X,x), Active, Duplicated(γ, dx))\n\nd1 = deriv_ll(D,X,γ)\nd2 = auto_deriv_ll(D,X,γ)\nauto_deriv2_ll(D,X,γ,dx)\n[d1 d2 dx]\n```\n\n::: {.cell-output .cell-output-display execution_count=10}\n```\n2×3 Matrix{Float64}:\n 18.585   18.585   18.585\n 36.7039  36.7039  36.7039\n```\n:::\n:::\n\n\nOk so we're confident that these functions work as intended, but how do they compare in performance?\n\n::: {#ffcc2ce4 .cell execution_count=10}\n``` {.julia .cell-code}\n@time deriv_ll(D,X,γ);\n@time auto_deriv_ll(D,X,γ);\n@time auto_deriv2_ll(D,X,γ,dx);\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n  0.000021 seconds (2 allocations: 80 bytes)\n  0.000035 seconds (7 allocations: 304 bytes)\n  0.000033 seconds\n```\n:::\n:::\n\n\nAll are quite quick and you can see that we're not losing much with automatic differentiation. In my experience, the gap between the two methods can narrow for more complicated functions.\n\nSo now let's try implementing the maximum likelihood estimator using two different gradient-based algorithms: Newton's Method (which uses the Hessian), and the Broyden–Fletcher–Goldfarb–Shannon (BFGS) algorithm (which updates search direction using changes in the first derivative). \n\nWhile Newton's method requires calculation of the Hessian (second derivatives), BFGS and related methods only require first derivatives. Typically, this makes each iteration quicker but will take more time to converge. Let's test them.\n\n::: {#c861895f .cell execution_count=11}\n``` {.julia .cell-code}\nusing Optim\nmin_objective(x) = -log_likelihood(D,X,x) #<- Optim assumes that we will minimize a function, hence the negative\nγ_guess = zeros(2)\nprintln(\" ---- Using Newton's Method ------ \")\nres1 = optimize(min_objective,γ_guess,Newton(),autodiff=:forward,Optim.Options(show_trace=true))\nprintln(\" ---- Using BFGS ------ \")\nres2 = optimize(min_objective,γ_guess,BFGS(),autodiff=:forward,Optim.Options(show_trace=true))\n[res1.minimizer res2.minimizer γ]\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n ---- Using Newton's Method ------ \nIter     Function value   Gradient norm \n     0     6.931472e+02     8.794794e+02\n * time: 0.011163949966430664\n     1     5.038529e+02     1.424353e+02\n * time: 0.3925299644470215\n     2     4.896962e+02     5.718137e+00\n * time: 0.39277005195617676\n     3     4.896776e+02     6.554352e-04\n * time: 0.39298009872436523\n     4     4.896776e+02     2.812263e-10\n * time: 0.3931429386138916\n ---- Using BFGS ------ \nIter     Function value   Gradient norm \n     0     6.931472e+02     8.794794e+02\n * time: 5.507469177246094e-5\n     1     5.022184e+02     1.277309e+02\n * time: 0.007002115249633789\n     2     4.993056e+02     1.210926e+02\n * time: 0.007193088531494141\n     3     4.898180e+02     1.538296e+01\n * time: 0.007416963577270508\n     4     4.896776e+02     7.292141e-02\n * time: 0.007581949234008789\n     5     4.896776e+02     2.642307e-03\n * time: 0.007750034332275391\n     6     4.896776e+02     6.079906e-08\n * time: 0.007950067520141602\n     7     4.896776e+02     3.867739e-14\n * time: 0.008134126663208008\n```\n:::\n\n::: {.cell-output .cell-output-display execution_count=12}\n```\n2×3 Matrix{Float64}:\n 0.143143  0.143143  0.1\n 0.540119  0.540119  0.5\n```\n:::\n:::\n\n\n## Further Reading\n\n- [JuliaDiff documentation](https://juliadiff.org/)\n- [ForwardDiff.jl docs](https://juliadiff.org/ForwardDiff.jl/stable/)\n- [Enzyme.jl docs](https://enzyme.mit.edu/julia/)\n\n",
    "supporting": [
      "autodiff_files"
    ],
    "filters": [],
    "includes": {}
  }
}